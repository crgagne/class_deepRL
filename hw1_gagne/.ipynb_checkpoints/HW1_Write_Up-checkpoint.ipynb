{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.1 Behavioral Cloning (Ant and Cheetah Envs )\n",
    "\n",
    "Environments: Cheetah-v1, Ant-v1\n",
    "\n",
    "Training Data: I created 98 rollouts with the cheetah expert and ant, capping each episode at 100 time steps. I capped at 100 time steps because I wanted to balance keeping the training data small with having alot of examples of the cheetah getting started - this seems like the hardest part to learn. This created 9700 (s,a) pairs for training for cheetah and ant.\n",
    "\n",
    "Network: 2 hidden layers with 64 units each (like the expert) and using tanh nonlinearity \n",
    "\n",
    "Training: I used mean square error loss. I did full gradient descent (ie. used the full training set for each gradient step). I also experimented a bit with batch gradient descent, which is probably the way to go. I normalized training observations using mean, std for entire dataset. I normalized the observations for testing in the gym. I did 10,000 gradient steps for each. I used a learning rate = 0.2\n",
    "\n",
    "Interleaving Gym with gradient steps: Every 10 gradient steps (still with the full dataset), I run an episode in the gym and store the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The network achieves somewhat 'comparable' performance for the Cheetah environment, but not for the Ant environment. If I fiddled with learning rate and kept running the gradient steps, cheetah keeps getting closer to expert. Ant seems to plateau. \n",
    "\n",
    "\n",
    "| Env| Mean Return (last 100 episodes)  | Std Return (last 100 episodes)  | Max Steps | Gradient Steps |Training Size ((s,a) pairs)|\n",
    "| ------------- |-------------|-----|\n",
    "| Cheetah | 210.1 | 44.7 |100 |10000 | 9700|\n",
    "| Cheetah Expert| 284.64 | 24.33 |100 |NA |NA|\n",
    "| ...| ...| ...|... |... |\n",
    "| Ant     | 222.79 | 70.51 |100 |10000 | 9700 | \n",
    "| Ant Expert     | 412.85 | 29.93 |100 | NA|NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.2 Experimenting with Number of Rollouts \n",
    "\n",
    "\n",
    "I wanted to try different amounts of training data (ie. number of rollouts). This will likely determine the level at which performance plateua's relative to the expert as there are more and more unique states observed. However, it'll likely take longer to achieve relatively good performance. \n",
    "\n",
    "### Results\n",
    "I tried 97 (blue), 197 (green), 297 (orange) episodes in the ant-v1. Surprisingly, the size of the training data had a miniminal effect on behavioral clonding's ability to get close to the expert. There is something else that is the issue here (learning rate, quality of training example, etc. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ant](ant_97_197_297.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 4. Dagger\n",
    "\n",
    "I've decide to implement dagger in the reacher environment. See my HW1_code.ipynb for the code. I've built both the dagger and behavioral cloning inside the same training function. This allowed me to experiment better. \n",
    "\n",
    "Behavioral Cloning:  I ran Reacher with 500 rollouts which generates a 25,000 (s,a) pair dataset.\n",
    "\n",
    "\n",
    "Interleaving Gym with gradient steps: I kept the same procedure as above: every 10 gradient steps (still with the full dataset), I run an episode in the gym and store the results. (I also tried running a gym episode for each gradient step). After each episode, I label the observations using the expert and append the dataset. The dataset grows with every 10 gradient steps then.\n",
    "\n",
    "\n",
    "### Results: \n",
    "\n",
    "Dagger does much better for reacher in terms of assymptotic performance. More importantly, it is more efficient. By the end of running the dagger algorithm, the total dataset is 19330 pairs, which is still <25,000 (s,a) pairs for behavioral cloning dataset, yet the performance is much higher for dagger. \n",
    "\n",
    "| Algorithm | Env| Mean Return (last 100 episodes)  | Std Return (last 100 episodes) | Gradient Steps | Training Size ((s,a) pairs)|\n",
    "| ------------- |-------------|-----|\n",
    "|behavioral cloning |Reacher | -10.27 | 5.63 | 3000 |25000 |\n",
    "|dagger | Reacher | -7.01 | 3.05| 3000 |4850 to start .. 19330 by end|\n",
    "|expert | Reacher | -3.64 | 1.78|NA |NA |\n",
    "\n",
    "\n",
    "![dagger](reacher_dagger_bc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

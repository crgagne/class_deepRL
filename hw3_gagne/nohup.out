[2017-09-30 22:08:23,503] Making new env: Pong-ram-v0
[2017-09-30 22:08:23,946] Clearing 14 monitor files from previous run (because force=True was provided)
2017-09-30 22:08:23.947759: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:08:23.947779: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:08:23.947787: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:08:24.045987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-30 22:08:24.046259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 3.94GiB
Free memory: 3.91GiB
2017-09-30 22:08:24.046283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-09-30 22:08:24.046293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-09-30 22:08:24.046305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
2017-09-30 22:08:24.089695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
('AVAILABLE GPUS: ', [u'device: 0, name: GRID K520, pci bus id: 0000:00:03.0'])
learning_starts
50000
[2017-09-30 22:08:24,402] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000000.mp4
last obs shape
(128,)

[2017-09-30 22:08:25,751] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000001.mp4
[2017-09-30 22:08:32,414] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000008.mp4
[2017-09-30 22:08:48,753] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000027.mp4
[2017-09-30 22:09:21,344] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000064.mp4
[2017-09-30 22:10:14,544] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000125.mp4
51000
52000
53000
54000
55000
56000
57000
58000
59000
60000
Timestep 60000
mean reward (100 episodes) -20.680000
best mean reward -20.400000
episodes 203
exploration 0.194000
learning_rate 0.000100
61000
62000
63000
[2017-09-30 22:11:51,026] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000216.mp4
64000
65000
66000
67000
68000
69000
70000
Timestep 70000
mean reward (100 episodes) -20.750000
best mean reward -20.400000
episodes 239
exploration 0.193000
learning_rate 0.000100
71000
72000
73000
74000
75000
76000
77000
78000
79000
80000
Timestep 80000
mean reward (100 episodes) -20.780000
best mean reward -20.400000
episodes 277
exploration 0.192000
learning_rate 0.000100
81000
82000
83000
84000
85000
86000
87000
88000
89000
90000
updating target network
Timestep 90000
mean reward (100 episodes) -20.850000
best mean reward -20.400000
episodes 316
exploration 0.191000
learning_rate 0.000100
91000
92000
93000
94000
95000
96000
97000
[2017-09-30 22:14:23,078] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000343.mp4
98000
99000
100000
Timestep 100000
mean reward (100 episodes) -20.880000
best mean reward -20.400000
episodes 353
exploration 0.190000
learning_rate 0.000100
101000
102000
103000
104000
105000
106000
107000
108000
109000
110000
Timestep 110000
mean reward (100 episodes) -20.880000
best mean reward -20.400000
episodes 389
exploration 0.189000
learning_rate 0.000100
111000
112000
113000
114000
115000
116000
117000
118000
119000
120000
Timestep 120000
mean reward (100 episodes) -20.830000
best mean reward -20.400000
episodes 425
exploration 0.188000
learning_rate 0.000100
121000
122000
123000
124000
125000
126000
127000
128000
129000
130000
updating target network
Timestep 130000
mean reward (100 episodes) -20.830000
best mean reward -20.400000
episodes 460
exploration 0.187000
learning_rate 0.000100
131000
132000
133000
134000
135000
136000
137000
138000
139000
140000
Timestep 140000
mean reward (100 episodes) -20.850000
best mean reward -20.400000
episodes 495
exploration 0.186000
learning_rate 0.000100
141000
142000
143000
144000
[2017-09-30 22:17:56,551] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000512.mp4
145000
146000
147000
148000
149000
150000
Timestep 150000
mean reward (100 episodes) -20.840000
best mean reward -20.400000
episodes 532
exploration 0.185000
learning_rate 0.000100
151000
152000
153000
154000
155000
156000
157000
158000
159000
160000
Timestep 160000
mean reward (100 episodes) -20.770000
best mean reward -20.400000
episodes 566
exploration 0.184000
learning_rate 0.000100
161000
162000
163000
164000
165000
166000
167000
168000
169000
170000
updating target network
Timestep 170000
mean reward (100 episodes) -20.700000
best mean reward -20.400000
episodes 599
exploration 0.183000
learning_rate 0.000100
171000
172000
173000
174000
175000
176000
177000
178000
179000
180000
Timestep 180000
mean reward (100 episodes) -20.600000
best mean reward -20.400000
episodes 630
exploration 0.182000
learning_rate 0.000100
181000
182000
183000
184000
185000
186000
187000
188000
189000
190000
Timestep 190000
mean reward (100 episodes) -20.570000
best mean reward -20.400000
episodes 660
exploration 0.181000
learning_rate 0.000100
191000
192000
193000
194000
195000
196000
197000
198000
199000
200000
Timestep 200000
mean reward (100 episodes) -20.520000
best mean reward -20.400000
episodes 690
exploration 0.180000
learning_rate 0.000100
201000
202000
203000
204000
205000
206000
207000
208000
209000
210000
updating target network
Timestep 210000
mean reward (100 episodes) -20.470000
best mean reward -20.400000
episodes 718
exploration 0.179000
learning_rate 0.000100
211000
212000
213000
[2017-09-30 22:23:05,875] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video000729.mp4
214000
215000
216000
217000
218000
219000
220000
Timestep 220000
mean reward (100 episodes) -20.570000
best mean reward -20.400000
episodes 748
exploration 0.178000
learning_rate 0.000100
221000
222000
223000
224000
225000
226000
227000
228000
229000
230000
Timestep 230000
mean reward (100 episodes) -20.500000
best mean reward -20.400000
episodes 777
exploration 0.177000
learning_rate 0.000100
231000
232000
233000
234000
235000
236000
237000
238000
239000
240000
Timestep 240000
mean reward (100 episodes) -20.610000
best mean reward -20.400000
episodes 805
exploration 0.176000
learning_rate 0.000100
241000
242000
243000
244000
245000
246000
247000
248000
249000
250000
updating target network
Timestep 250000
mean reward (100 episodes) -20.450000
best mean reward -20.400000
episodes 830
exploration 0.175000
learning_rate 0.000100
251000
252000
253000
254000
255000
256000
257000
258000
259000
260000
Timestep 260000
mean reward (100 episodes) -20.390000
best mean reward -20.360000
episodes 858
exploration 0.174000
learning_rate 0.000100
261000
262000
263000
264000
265000
266000
267000
268000
269000
270000
Timestep 270000
mean reward (100 episodes) -20.300000
best mean reward -20.290000
episodes 885
exploration 0.173000
learning_rate 0.000100
271000
272000
273000
274000
275000
276000
277000
278000
279000
280000
Timestep 280000
mean reward (100 episodes) -20.240000
best mean reward -20.190000
episodes 913
exploration 0.172000
learning_rate 0.000100
281000
282000
283000
284000
285000
286000
287000
288000
289000
290000
updating target network
Timestep 290000
mean reward (100 episodes) -20.340000
best mean reward -20.190000
episodes 940
exploration 0.171000
learning_rate 0.000100
291000
292000
293000
294000
295000
296000
297000
298000
299000
300000
Timestep 300000
mean reward (100 episodes) -20.280000
best mean reward -20.190000
episodes 968
exploration 0.170000
learning_rate 0.000100
301000
302000
303000
304000
305000
306000
307000
308000
309000
310000
Timestep 310000
mean reward (100 episodes) -20.320000
best mean reward -20.190000
episodes 993
exploration 0.169000
learning_rate 0.000100
311000
312000
[2017-09-30 22:30:28,757] Starting new video recorder writing to /tmp/hw3_vid_dir/gym/openaigym.video.0.18620.video001000.mp4
313000
314000
315000
316000
317000
318000
319000
320000
Timestep 320000
mean reward (100 episodes) -20.300000
best mean reward -20.190000
episodes 1018
exploration 0.168000
learning_rate 0.000100
321000
322000
323000
324000
325000
326000
327000
328000
329000
330000
updating target network
Timestep 330000
mean reward (100 episodes) -20.250000
best mean reward -20.190000
episodes 1044
exploration 0.167000
learning_rate 0.000100
331000
332000
333000
334000
335000
336000
337000
338000
339000
340000
Timestep 340000
mean reward (100 episodes) -20.200000
best mean reward -20.190000
episodes 1068
exploration 0.166000
learning_rate 0.000100
341000
342000
343000
344000
345000
346000
347000
348000
349000
350000
Timestep 350000
mean reward (100 episodes) -20.090000
best mean reward -20.080000
episodes 1091
exploration 0.165000
learning_rate 0.000100
351000
352000
353000
354000
355000
356000
357000
358000
359000
360000
Timestep 360000
mean reward (100 episodes) -19.980000
best mean reward -19.940000
episodes 1115
exploration 0.164000
learning_rate 0.000100
361000
362000
363000
364000
365000
366000
367000
368000
369000
370000
updating target network
Timestep 370000
mean reward (100 episodes) -19.920000
best mean reward -19.890000
episodes 1138
exploration 0.163000
learning_rate 0.000100
371000
372000
373000
374000
375000
376000
377000
378000
379000
380000
Timestep 380000
mean reward (100 episodes) -19.800000
best mean reward -19.760000
episodes 1163
exploration 0.162000
learning_rate 0.000100
381000
382000
383000
384000
385000
386000
387000
388000
389000
390000
Timestep 390000
mean reward (100 episodes) -19.870000
best mean reward -19.760000
episodes 1187
exploration 0.161000
learning_rate 0.000100
391000
392000
393000
394000
395000
396000
397000
398000
399000
400000
Timestep 400000
mean reward (100 episodes) -19.890000
best mean reward -19.760000
episodes 1211
exploration 0.160000
learning_rate 0.000100
401000
402000
403000
404000
405000
406000
407000
408000
409000
410000
updating target network
Timestep 410000
mean reward (100 episodes) -19.910000
best mean reward -19.760000
episodes 1233
exploration 0.159000
learning_rate 0.000100
411000
412000
413000
414000
415000
416000
417000
418000
419000
420000
Timestep 420000
mean reward (100 episodes) -19.970000
best mean reward -19.760000
episodes 1256
exploration 0.158000
learning_rate 0.000100
421000
422000
423000
424000
425000
426000
427000
428000
429000
430000
Timestep 430000
mean reward (100 episodes) -19.910000
best mean reward -19.760000
episodes 1276
exploration 0.157000
learning_rate 0.000100
431000
432000
433000
434000
435000
436000
437000
438000
439000
440000
Timestep 440000
mean reward (100 episodes) -19.880000
best mean reward -19.760000
episodes 1297
exploration 0.156000
learning_rate 0.000100
441000
442000
443000
444000
445000
446000
447000
448000
449000
450000
updating target network
Timestep 450000
mean reward (100 episodes) -19.810000
best mean reward -19.760000
episodes 1319
exploration 0.155000
learning_rate 0.000100
451000
452000
453000
454000
455000
456000
457000
458000
459000
460000
Timestep 460000
mean reward (100 episodes) -19.730000
best mean reward -19.700000
episodes 1339
exploration 0.154000
learning_rate 0.000100
461000
462000
463000
464000
465000
466000
467000
468000
469000
470000
Timestep 470000
mean reward (100 episodes) -19.650000
best mean reward -19.640000
episodes 1359
exploration 0.153000
learning_rate 0.000100
471000
472000
473000
474000
475000
476000
477000
478000
479000
480000
Timestep 480000
mean reward (100 episodes) -19.500000
best mean reward -19.490000
episodes 1380
exploration 0.152000
learning_rate 0.000100
481000
482000
483000
484000
485000
486000
487000
488000
489000
490000
updating target network
Timestep 490000
mean reward (100 episodes) -19.480000
best mean reward -19.460000
episodes 1399
exploration 0.151000
learning_rate 0.000100
491000
492000
493000
494000
495000
496000
497000
498000
499000
500000
Timestep 500000
mean reward (100 episodes) -19.510000
best mean reward -19.430000
episodes 1420
exploration 0.150000
learning_rate 0.000100
501000
502000
503000
504000
505000
506000
507000
508000
509000
510000
Timestep 510000
mean reward (100 episodes) -19.620000
best mean reward -19.430000
episodes 1439
exploration 0.149000
learning_rate 0.000100
511000
512000
513000
514000
515000
516000
517000
518000
519000
520000
Timestep 520000
mean reward (100 episodes) -19.620000
best mean reward -19.430000
episodes 1459
exploration 0.148000
learning_rate 0.000100
521000
522000
523000
524000
525000
526000
527000
528000
529000
530000
updating target network
Timestep 530000
mean reward (100 episodes) -19.650000
best mean reward -19.430000
episodes 1478
exploration 0.147000
learning_rate 0.000100
531000
532000
533000
534000
535000
536000
537000
538000
539000
540000
Timestep 540000
mean reward (100 episodes) -19.690000
best mean reward -19.430000
episodes 1497
exploration 0.146000
learning_rate 0.000100
541000
542000
543000
544000
545000
546000
547000
548000
549000
550000
Timestep 550000
mean reward (100 episodes) -19.610000
best mean reward -19.430000
episodes 1515
exploration 0.145000
learning_rate 0.000100
551000
552000
553000
554000
555000
556000
557000
558000
559000
560000
Timestep 560000
mean reward (100 episodes) -19.440000
best mean reward -19.430000
episodes 1534
exploration 0.144000
learning_rate 0.000100
561000
562000
563000
564000
565000
566000
567000
568000
569000
570000
updating target network
Timestep 570000
mean reward (100 episodes) -19.510000
best mean reward -19.430000
episodes 1552
exploration 0.143000
learning_rate 0.000100
571000
572000
573000
[2017-09-30 22:55:06,560] Making new env: PongNoFrameskip-v4
[2017-09-30 22:55:07,007] Clearing 8 monitor files from previous run (because force=True was provided)
2017-09-30 22:55:07.008561: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:55:07.008581: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:55:07.008590: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:55:07.110911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-30 22:55:07.111182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 3.94GiB
Free memory: 3.91GiB
2017-09-30 22:55:07.111206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-09-30 22:55:07.111215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-09-30 22:55:07.111228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
2017-09-30 22:55:07.153782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
('AVAILABLE GPUS: ', [u'device: 0, name: GRID K520, pci bus id: 0000:00:03.0'])
learning_starts
500000
env observation space
(84, 84, 1)
[2017-09-30 22:55:07,555] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19550.video000000.mp4
last obs shape
(84, 84, 1)

[2017-09-30 22:55:10,424] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19550.video000001.mp4
[2017-09-30 22:55:23,820] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19550.video000008.mp4
[2017-09-30 22:55:56,243] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19550.video000027.mp4
[2017-09-30 22:56:57,995] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19550.video000064.mp4
Traceback (most recent call last):
  File "run_dqn_atari.py", line 133, in <module>
    main()
  File "run_dqn_atari.py", line 130, in main
    atari_learn(env, session, num_timesteps=task.max_timesteps)
  File "run_dqn_atari.py", line 77, in atari_learn
    grad_norm_clipping=10
  File "/home/ubuntu/hw3_gagne/dqn.py", line 271, in learn
    obs, reward, done, info = env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/hw3_gagne/atari_wrappers.py", line 128, in _step
    obs, reward, done, info = self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/hw3_gagne/atari_wrappers.py", line 120, in _step
    obs, reward, done, info = self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 280, in _step
    return self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/hw3_gagne/atari_wrappers.py", line 89, in _step
    obs, reward, done, info = self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 280, in _step
    return self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/hw3_gagne/atari_wrappers.py", line 49, in _step
    obs, reward, done, info = self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/wrappers/monitoring.py", line 33, in _step
    observation, reward, done, info = self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/wrappers/time_limit.py", line 36, in _step
    observation, reward, done, info = self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/envs/atari/atari_env.py", line 80, in _step
    reward += self.ale.act(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/atari_py/ale_python_interface.py", line 136, in act
    return ale_lib.act(self.obj, int(action))
KeyboardInterrupt
[2017-09-30 22:57:55,824] Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/hw3_vid_dir2/gym')
[2017-09-30 22:58:13,934] Making new env: PongNoFrameskip-v4
[2017-09-30 22:58:14,380] Clearing 12 monitor files from previous run (because force=True was provided)
2017-09-30 22:58:14.381755: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:58:14.381785: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:58:14.381794: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 22:58:14.485217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-30 22:58:14.485529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 3.94GiB
Free memory: 3.91GiB
2017-09-30 22:58:14.485556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-09-30 22:58:14.485576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-09-30 22:58:14.485591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
2017-09-30 22:58:14.536006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
('AVAILABLE GPUS: ', [u'device: 0, name: GRID K520, pci bus id: 0000:00:03.0'])
learning_starts
500000
env observation space
(84, 84, 1)
[2017-09-30 22:58:14,938] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19693.video000000.mp4
last obs shape
(84, 84, 1)

[2017-09-30 22:58:17,803] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19693.video000001.mp4
[2017-09-30 22:58:31,006] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19693.video000008.mp4
[2017-09-30 22:59:03,106] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19693.video000027.mp4
[2017-09-30 23:00:04,318] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19693.video000064.mp4
[2017-09-30 23:01:40,542] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19693.video000125.mp4
Traceback (most recent call last):
  File "run_dqn_atari.py", line 133, in <module>
    main()
  File "run_dqn_atari.py", line 130, in main
    atari_learn(env, session, num_timesteps=task.max_timesteps)
  File "run_dqn_atari.py", line 77, in atari_learn
    grad_norm_clipping=10
  File "/home/ubuntu/hw3_gagne/dqn.py", line 271, in learn
    obs, reward, done, info = env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/hw3_gagne/atari_wrappers.py", line 128, in _step
    obs, reward, done, info = self.env.step(action)
  File "/home/ubuntu/.local/lib/python2.7/site-packages/gym/core.py", line 96, in step
    return self._step(action)
  File "/home/ubuntu/hw3_gagne/atari_wrappers.py", line 121, in _step
    return _process_frame84(obs), reward, done, info
  File "/home/ubuntu/hw3_gagne/atari_wrappers.py", line 108, in _process_frame84
    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114
KeyboardInterrupt
[2017-09-30 23:02:05,800] Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/hw3_vid_dir2/gym')
[2017-09-30 23:02:09,917] Making new env: PongNoFrameskip-v4
[2017-09-30 23:02:10,361] Clearing 14 monitor files from previous run (because force=True was provided)
2017-09-30 23:02:10.363020: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 23:02:10.363040: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 23:02:10.363058: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 23:02:10.472042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-09-30 23:02:10.472317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 3.94GiB
Free memory: 3.91GiB
2017-09-30 23:02:10.472342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-09-30 23:02:10.472351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-09-30 23:02:10.472364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
2017-09-30 23:02:10.517194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
('AVAILABLE GPUS: ', [u'device: 0, name: GRID K520, pci bus id: 0000:00:03.0'])
learning_starts
500000
env observation space
(84, 84, 1)
[2017-09-30 23:02:10,928] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000000.mp4
last obs shape
(84, 84, 1)

[2017-09-30 23:02:13,831] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000001.mp4
[2017-09-30 23:02:26,907] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000008.mp4
[2017-09-30 23:02:58,758] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000027.mp4
[2017-09-30 23:03:59,215] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000064.mp4
[2017-09-30 23:05:35,677] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000125.mp4
[2017-09-30 23:08:00,490] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000216.mp4
[2017-09-30 23:11:23,452] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000343.mp4
[2017-09-30 23:15:44,005] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000512.mp4
501000
502000
503000
504000
505000
506000
507000
508000
509000
510000
Timestep 510000
mean reward (100 episodes) -20.380000
best mean reward -20.210000
episodes 557
exploration 0.541000
learning_rate 0.000100
511000
512000
513000
514000
515000
516000
517000
518000
519000
520000
Timestep 520000
mean reward (100 episodes) -20.340000
best mean reward -20.210000
episodes 568
exploration 0.532000
learning_rate 0.000100
521000
522000
523000
524000
525000
526000
527000
528000
529000
530000
Timestep 530000
mean reward (100 episodes) -20.370000
best mean reward -20.210000
episodes 579
exploration 0.523000
learning_rate 0.000100
531000
532000
533000
534000
535000
536000
537000
538000
539000
540000
updating target network
Timestep 540000
mean reward (100 episodes) -20.350000
best mean reward -20.210000
episodes 590
exploration 0.514000
learning_rate 0.000100
541000
542000
543000
544000
545000
546000
547000
548000
549000
550000
Timestep 550000
mean reward (100 episodes) -20.410000
best mean reward -20.210000
episodes 601
exploration 0.505000
learning_rate 0.000100
551000
552000
553000
554000
555000
556000
557000
558000
559000
560000
Timestep 560000
mean reward (100 episodes) -20.380000
best mean reward -20.210000
episodes 612
exploration 0.496000
learning_rate 0.000100
561000
562000
563000
564000
565000
566000
567000
568000
569000
570000
Timestep 570000
mean reward (100 episodes) -20.390000
best mean reward -20.210000
episodes 623
exploration 0.487000
learning_rate 0.000100
571000
572000
573000
574000
575000
576000
577000
578000
579000
580000
updating target network
Timestep 580000
mean reward (100 episodes) -20.400000
best mean reward -20.210000
episodes 634
exploration 0.478000
learning_rate 0.000100
581000
582000
583000
584000
585000
586000
587000
588000
589000
590000
Timestep 590000
mean reward (100 episodes) -20.380000
best mean reward -20.210000
episodes 644
exploration 0.469000
learning_rate 0.000100
591000
592000
593000
594000
595000
596000
597000
598000
599000
600000
Timestep 600000
mean reward (100 episodes) -20.370000
best mean reward -20.210000
episodes 655
exploration 0.460000
learning_rate 0.000100
601000
602000
603000
604000
605000
606000
607000
608000
609000
610000
Timestep 610000
mean reward (100 episodes) -20.390000
best mean reward -20.210000
episodes 666
exploration 0.451000
learning_rate 0.000100
611000
612000
613000
614000
615000
616000
617000
618000
619000
620000
updating target network
Timestep 620000
mean reward (100 episodes) -20.370000
best mean reward -20.210000
episodes 678
exploration 0.442000
learning_rate 0.000100
621000
622000
623000
624000
625000
626000
627000
628000
629000
630000
Timestep 630000
mean reward (100 episodes) -20.320000
best mean reward -20.210000
episodes 688
exploration 0.433000
learning_rate 0.000100
631000
632000
633000
634000
635000
636000
637000
638000
639000
640000
Timestep 640000
mean reward (100 episodes) -20.330000
best mean reward -20.210000
episodes 699
exploration 0.424000
learning_rate 0.000100
641000
642000
643000
644000
645000
646000
647000
648000
649000
650000
Timestep 650000
mean reward (100 episodes) -20.340000
best mean reward -20.210000
episodes 710
exploration 0.415000
learning_rate 0.000100
651000
652000
653000
654000
655000
656000
657000
658000
659000
660000
updating target network
Timestep 660000
mean reward (100 episodes) -20.320000
best mean reward -20.210000
episodes 722
exploration 0.406000
learning_rate 0.000100
661000
662000
663000
664000
665000
666000
[2017-09-30 23:49:56,065] Starting new video recorder writing to /tmp/hw3_vid_dir2/gym/openaigym.video.0.19840.video000729.mp4
667000
668000
669000
670000
Timestep 670000
mean reward (100 episodes) -20.260000
best mean reward -20.210000
episodes 733
exploration 0.397000
learning_rate 0.000100
671000
672000
673000
674000
675000
676000
677000
678000
679000
680000
Timestep 680000
mean reward (100 episodes) -20.240000
best mean reward -20.210000
episodes 743
exploration 0.388000
learning_rate 0.000100
681000
682000
683000
684000
685000
686000
687000
688000
689000
690000
Timestep 690000
mean reward (100 episodes) -20.250000
best mean reward -20.210000
episodes 755
exploration 0.379000
learning_rate 0.000100
691000
692000
693000
694000
695000
696000
697000
698000
699000
700000
updating target network
Timestep 700000
mean reward (100 episodes) -20.220000
best mean reward -20.210000
episodes 766
exploration 0.370000
learning_rate 0.000100
701000
702000
703000
704000
705000
706000
707000
708000
709000
710000
Timestep 710000
mean reward (100 episodes) -20.170000
best mean reward -20.170000
episodes 776
exploration 0.361000
learning_rate 0.000100
711000
712000
713000
714000
715000
716000
717000
718000
719000
720000
Timestep 720000
mean reward (100 episodes) -20.140000
best mean reward -20.110000
episodes 787
exploration 0.352000
learning_rate 0.000100
721000
722000
723000
724000
725000
726000
727000
728000
729000
730000
Timestep 730000
mean reward (100 episodes) -20.060000
best mean reward -20.060000
episodes 797
exploration 0.343000
learning_rate 0.000100
731000
732000
733000
734000
735000
736000
737000
738000
739000
740000
updating target network
Timestep 740000
mean reward (100 episodes) -20.040000
best mean reward -20.040000
episodes 808
exploration 0.334000
learning_rate 0.000100
741000
742000
743000
744000
745000
746000
747000
748000
749000
750000
Timestep 750000
mean reward (100 episodes) -19.920000
best mean reward -19.920000
episodes 818
exploration 0.325000
learning_rate 0.000100
751000
752000
753000
754000
755000
756000
757000
758000
759000
760000
Timestep 760000
mean reward (100 episodes) -19.920000
best mean reward -19.900000
episodes 829
exploration 0.316000
learning_rate 0.000100
761000
762000
763000
764000
765000
766000
767000
768000
769000
770000
Timestep 770000
mean reward (100 episodes) -19.970000
best mean reward -19.900000
episodes 839
exploration 0.307000
learning_rate 0.000100
771000
772000
773000
774000
775000
776000
777000
778000
779000
780000
updating target network
Timestep 780000
mean reward (100 episodes) -19.950000
best mean reward -19.900000
episodes 849
exploration 0.298000
learning_rate 0.000100
781000
782000
783000
784000
785000
786000
787000
788000
789000
790000
Timestep 790000
mean reward (100 episodes) -19.900000
best mean reward -19.890000
episodes 859
exploration 0.289000
learning_rate 0.000100
791000
792000
793000
794000
795000
796000
797000
798000
799000
800000
Timestep 800000
mean reward (100 episodes) -19.950000
best mean reward -19.890000
episodes 870
exploration 0.280000
learning_rate 0.000100
801000
802000
803000
804000
805000
806000
807000
808000
809000
810000
Timestep 810000
mean reward (100 episodes) -19.970000
best mean reward -19.890000
episodes 881
exploration 0.271000
learning_rate 0.000100
811000
812000
813000
814000
815000
816000
817000
818000
819000
820000
updating target network
Timestep 820000
mean reward (100 episodes) -19.950000
best mean reward -19.890000
episodes 891
exploration 0.262000
learning_rate 0.000100
821000
822000
823000
824000
825000
826000
827000
828000
829000
830000
Timestep 830000
mean reward (100 episodes) -19.930000
best mean reward -19.890000
episodes 901
exploration 0.253000
learning_rate 0.000100
831000
832000
833000
834000
835000
836000
837000
838000
839000
840000
Timestep 840000
mean reward (100 episodes) -20.010000
best mean reward -19.890000
episodes 911
exploration 0.244000
learning_rate 0.000100
841000
842000
843000
844000
845000
846000
847000
848000
849000
850000
Timestep 850000
mean reward (100 episodes) -19.940000
best mean reward -19.890000
episodes 921
exploration 0.235000
learning_rate 0.000100
851000
852000
853000
854000
855000
856000
857000
858000
859000
860000
updating target network
Timestep 860000
mean reward (100 episodes) -19.900000
best mean reward -19.890000
episodes 931
exploration 0.226000
learning_rate 0.000100
861000
862000
863000
864000
865000
866000
867000
868000
869000
870000
Timestep 870000
mean reward (100 episodes) -19.700000
best mean reward -19.700000
episodes 939
exploration 0.217000
learning_rate 0.000100
871000
872000
873000
874000
875000
876000
877000
878000
879000
880000
Timestep 880000
mean reward (100 episodes) -19.610000
best mean reward -19.610000
episodes 948
exploration 0.208000
learning_rate 0.000100
881000
882000
883000
884000
885000
886000
887000
888000
889000
890000
Timestep 890000
mean reward (100 episodes) -19.530000
best mean reward -19.530000
episodes 957
exploration 0.199000
learning_rate 0.000100
891000
892000
893000
894000
895000
896000
897000
898000
899000
900000
updating target network
Timestep 900000
mean reward (100 episodes) -19.380000
best mean reward -19.380000
episodes 965
exploration 0.190000
learning_rate 0.000100
901000
902000
903000
904000
905000
906000
907000
908000
909000
910000
Timestep 910000
mean reward (100 episodes) -19.270000
best mean reward -19.260000
episodes 973
exploration 0.181000
learning_rate 0.000100
911000
912000
913000
914000
915000
916000
917000
918000
919000
920000
Timestep 920000
mean reward (100 episodes) -19.130000
best mean reward -19.130000
episodes 981
exploration 0.172000
learning_rate 0.000100
921000
922000
923000
924000
925000
926000
927000
928000
929000
930000
Timestep 930000
mean reward (100 episodes) -18.990000
best mean reward -18.990000
episodes 988
exploration 0.163000
learning_rate 0.000100
931000
932000
933000
934000
